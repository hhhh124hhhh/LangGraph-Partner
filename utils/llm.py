"""
LLMé…ç½®æ¨¡å—
ç»Ÿä¸€ç®¡ç†å¤§è¯­è¨€æ¨¡å‹çš„åˆå§‹åŒ–å’Œé…ç½®
"""

import os
from openai import OpenAI
from dotenv import load_dotenv
import json

# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œå°è¯•åŠ è½½.envæ–‡ä»¶å’Œ.env.localæ–‡ä»¶
load_dotenv('.env')
load_dotenv('.env.local')

# æ‰“å°ç¯å¢ƒå˜é‡åŠ è½½çŠ¶æ€
print(f"ç¯å¢ƒå˜é‡åŠ è½½çŠ¶æ€:")
print(f"ZHIPU_API_KEY: {'å·²è®¾ç½®' if os.getenv('ZHIPU_API_KEY') else 'æœªè®¾ç½®'}")
print(f"ZHIPU_MODEL: {'å·²è®¾ç½® - ' + os.getenv('ZHIPU_MODEL') if os.getenv('ZHIPU_MODEL') else 'æœªè®¾ç½® (ä½¿ç”¨é»˜è®¤å€¼)'}")
print(f"AI_CLAUDE_API_KEY: {'å·²è®¾ç½®' if os.getenv('AI_CLAUDE_API_KEY') else 'æœªè®¾ç½®'}")
print(f"AI_CLAUDE_BASE_URL: {'å·²è®¾ç½®' if os.getenv('AI_CLAUDE_BASE_URL') else 'æœªè®¾ç½®'}")
print(f"OPENAI_API_KEY: {'å·²è®¾ç½®' if os.getenv('OPENAI_API_KEY') else 'æœªè®¾ç½®'}")
print(f"OPENAI_BASE_URL: {'å·²è®¾ç½®' if os.getenv('OPENAI_BASE_URL') else 'æœªè®¾ç½®'}")

class CustomLLM:
    """è‡ªå®šä¹‰LLMç±»ï¼Œç›´æ¥ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨API"""
    def __init__(self, model="glm-3-turbo", temperature=0.7):
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.api_key = os.getenv('ZHIPU_API_KEY')
        # è®¾ç½®å®Œæ•´çš„API URLè·¯å¾„
        self.base_url = "https://open.bigmodel.cn/api/paas/v4"
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹åç§°ï¼Œå¦åˆ™ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
        self.model_name = os.getenv('ZHIPU_MODEL', model)  # ä»ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹
        self.temperature = temperature
        
        print(f"åˆ›å»ºCustomLLMå®ä¾‹:")
        print(f"æ¨¡å‹: {self.model_name}")
        print(f"æ¸©åº¦: {self.temperature}")
        print(f"APIå¯†é’¥: {self.api_key[:8]}..." if self.api_key else "æœªè®¾ç½®")
        print(f"åŸºç¡€URL: {self.base_url}")
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
    
    def invoke(self, messages):
        """è°ƒç”¨APIå¹¶å¤„ç†å“åº”"""
        try:
            print(f"CustomLLMè°ƒç”¨ä¸­ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
            print(f"ä½¿ç”¨æ¨¡å‹: {self.model_name}")

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºæ™ºè°±AIå…¼å®¹çš„æ ¼å¼
            formatted_messages = []
            for msg in messages:
                if hasattr(msg, 'content') and hasattr(msg, 'type'):
                    # LangChain message format
                    content = msg.content
                    role = "user" if msg.type == "human" else "assistant" if msg.type == "ai" else "system"
                elif isinstance(msg, dict):
                    content = msg.get("content", "")
                    role = msg.get("role", "user")
                else:
                    content = str(msg)
                    role = "user"

                formatted_messages.append({
                    "role": role,
                    "content": content
                })

            # è°ƒç”¨API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=formatted_messages,
                temperature=self.temperature
            )
            
            # æå–å“åº”å†…å®¹
            if hasattr(response, 'choices') and response.choices:
                # æ ‡å‡†OpenAIæ ¼å¼
                content = response.choices[0].message.content
                print(f"æˆåŠŸæå–å“åº”å†…å®¹: {content[:50]}..." if content else "æ— å†…å®¹")
                # è¿”å›ä¸LangChainå…¼å®¹çš„æ ¼å¼
                return {"content": content, "role": "assistant"}
            else:
                print("å°è¯•å…¶ä»–æ ¼å¼æå–å†…å®¹...")
                return {"content": "APIè°ƒç”¨æˆåŠŸä½†æ— æ³•æå–å“åº”å†…å®¹", "role": "assistant"}
                
        except Exception as e:
            print(f"CustomLLMè°ƒç”¨é”™è¯¯: {e}")
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            error_msg = str(e)
            if "401" in error_msg:
                print("âŒ è®¤è¯å¤±è´¥: è¯·æ£€æŸ¥ZHIPU_API_KEYæ˜¯å¦æ­£ç¡®æœ‰æ•ˆ")
                print("   æç¤º: æ™ºè°±AIçš„APIå¯†é’¥æ ¼å¼é€šå¸¸ä¸º 'xxx.xxx'ï¼ŒåŒ…å«ä¸¤éƒ¨åˆ†")
            elif "404" in error_msg:
                print("âŒ æ¨¡å‹ä¸å­˜åœ¨: è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                print("   æ¨èä½¿ç”¨å…è´¹æ¨¡å‹: glm-3-turbo")
                print("   å…¶ä»–å¯èƒ½çš„æ¨¡å‹: glm-4, glm-4-flash")
            elif "429" in error_msg and "ä½™é¢ä¸è¶³" in error_msg:
                print("ğŸ’¡ æç¤º: è¯·å°è¯•ä½¿ç”¨æ™ºè°±AIçš„å…è´¹æ¨¡å‹ 'glm-3-turbo'")
                print("   æˆ–è€…å……å€¼æ‚¨çš„æ™ºè°±AIè´¦æˆ·ä»¥ä½¿ç”¨é«˜çº§æ¨¡å‹")
            # ç»§ç»­æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒåŸæœ‰é”™è¯¯å¤„ç†æµç¨‹
            raise

def get_llm(model="glm-4.6", temperature=0.7):
    """
    è·å–é…ç½®å¥½çš„LLMå®ä¾‹

    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨glm-4.6
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›ç­”çš„éšæœºæ€§

    Returns:
        CustomLLM: é…ç½®å¥½çš„LLMå®ä¾‹
    """
    return CustomLLM(model=model, temperature=temperature)

# é¢„å®šä¹‰çš„LLMå®ä¾‹
chat_llm = get_llm(temperature=0.7)
reasoning_llm = get_llm(temperature=0.1)