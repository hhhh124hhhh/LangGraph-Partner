#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangGraphä»£ç†æµ‹è¯•æ¡†æ¶

åŸºäºContext7è°ƒç ”çš„ä¼ä¸šçº§æµ‹è¯•å·¥å…·ï¼Œæä¾›å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€
æ€§èƒ½æµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•åŠŸèƒ½ã€‚
"""

import asyncio
import json
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, asdict
import argparse

try:
    import pytest
    import aiohttp
    from langchain_core.messages import HumanMessage, AIMessage
    from langgraph.graph import StateGraph, MessageGraph
    from langgraph.checkpoint.memory import MemorySaver
    LANGGRAPH_AVAILABLE = True
except ImportError as e:
    print(f"[WARNING] ç¼ºå°‘å¿…è¦ä¾èµ–: {e}")
    LANGGRAPH_AVAILABLE = False


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹æ•°æ®ç±»"""
    name: str
    description: str
    input_data: Dict[str, Any]
    expected_output: Optional[Dict[str, Any]] = None
    timeout: float = 30.0
    category: str = "general"


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    status: str  # "passed", "failed", "timeout", "error"
    duration: float
    output: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    assertion_errors: List[str] = None


class AgentTester:
    """LangGraphä»£ç†æµ‹è¯•å™¨"""

    def __init__(self, graph_or_agent, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨

        Args:
            graph_or_agent: LangGraphå›¾æˆ–ä»£ç†å®ä¾‹
            config: æµ‹è¯•é…ç½®
        """
        self.graph_or_agent = graph_or_agent
        self.config = config or {}
        self.test_cases: List[TestCase] = []
        self.test_results: List[TestResult] = []
        self.test_data: Dict[str, Any] = {}

    def add_test_case(self, test_case: TestCase):
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹"""
        self.test_cases.append(test_case)

    def add_test_cases_from_file(self, file_path: str):
        """ä»æ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            for test_data in data.get('test_cases', []):
                test_case = TestCase(**test_data)
                self.add_test_case(test_case)

            self.test_data = data
            print(f"[INFO] ä» {file_path} åŠ è½½äº† {len(data.get('test_cases', []))} ä¸ªæµ‹è¯•ç”¨ä¾‹")

        except Exception as e:
            print(f"[ERROR] åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")

    async def run_single_test(self, test_case: TestCase) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        start_time = time.time()

        try:
            # æ‰§è¡Œæµ‹è¯•
            if hasattr(self.graph_or_agent, 'ainvoke'):
                # LangGraphå›¾
                result = await asyncio.wait_for(
                    self.graph_or_agent.ainvoke(
                        test_case.input_data,
                        config=self.config
                    ),
                    timeout=test_case.timeout
                )
            elif hasattr(self.graph_or_agent, '__call__'):
                # å¯è°ƒç”¨å¯¹è±¡
                result = await asyncio.wait_for(
                    self.graph_or_agent(test_case.input_data),
                    timeout=test_case.timeout
                )
            else:
                # åŒæ­¥è°ƒç”¨
                result = await asyncio.wait_for(
                    asyncio.to_thread(
                        self.graph_or_agent,
                        test_case.input_data
                    ),
                    timeout=test_case.timeout
                )

            duration = time.time() - start_time

            # éªŒè¯é¢„æœŸè¾“å‡º
            assertion_errors = []
            if test_case.expected_output:
                assertion_errors = self._verify_output(result, test_case.expected_output)

            status = "failed" if assertion_errors else "passed"

            return TestResult(
                test_name=test_case.name,
                status=status,
                duration=duration,
                output=result,
                assertion_errors=assertion_errors
            )

        except asyncio.TimeoutError:
            duration = time.time() - start_time
            return TestResult(
                test_name=test_case.name,
                status="timeout",
                duration=duration,
                error_message=f"æµ‹è¯•è¶…æ—¶ï¼ˆ{test_case.timeout}ç§’ï¼‰"
            )

        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=test_case.name,
                status="error",
                duration=duration,
                error_message=str(e),
                assertion_errors=None
            )

    def _verify_output(self, actual: Any, expected: Dict[str, Any]) -> List[str]:
        """éªŒè¯è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸ"""
        errors = []

        try:
            if 'contains' in expected:
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«æŒ‡å®šå†…å®¹
                for item in expected['contains']:
                    if isinstance(item, str):
                        if item.lower() not in str(actual).lower():
                            errors.append(f"è¾“å‡ºä¸­æœªæ‰¾åˆ°é¢„æœŸå†…å®¹: {item}")
                    else:
                        if item not in str(actual):
                            errors.append(f"è¾“å‡ºä¸­æœªæ‰¾åˆ°é¢„æœŸå†…å®¹: {item}")

            if 'equals' in expected:
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ç­‰äºé¢„æœŸå€¼
                if actual != expected['equals']:
                    errors.append(f"è¾“å‡ºä¸ç­‰äºé¢„æœŸå€¼")

            if 'type' in expected:
                # æ£€æŸ¥è¾“å‡ºç±»å‹
                expected_type = expected['type']
                if expected_type == 'str' and not isinstance(actual, str):
                    errors.append(f"è¾“å‡ºç±»å‹ä¸æ˜¯å­—ç¬¦ä¸²: {type(actual)}")
                elif expected_type == 'dict' and not isinstance(actual, dict):
                    errors.append(f"è¾“å‡ºç±»å‹ä¸æ˜¯å­—å…¸: {type(actual)}")
                elif expected_type == 'list' and not isinstance(actual, list):
                    errors.append(f"è¾“å‡ºç±»å‹ä¸æ˜¯åˆ—è¡¨: {type(actual)}")

            if 'min_length' in expected:
                # æ£€æŸ¥æœ€å°é•¿åº¦
                min_length = expected['min_length']
                if isinstance(actual, (str, list)):
                    if len(actual) < min_length:
                        errors.append(f"è¾“å‡ºé•¿åº¦å°äºæœ€å°è¦æ±‚: {len(actual)} < {min_length}")
                elif isinstance(actual, dict):
                    if len(actual) < min_length:
                        errors.append(f"è¾“å‡ºé¡¹æ•°å°‘äºæœ€å°è¦æ±‚: {len(actual)} < {min_length}")

            if 'max_length' in expected:
                # æ£€æŸ¥æœ€å¤§é•¿åº¦
                max_length = expected['max_length']
                if isinstance(actual, (str, list)):
                    if len(actual) > max_length:
                        errors.append(f"è¾“å‡ºé•¿åº¦è¶…è¿‡æœ€å¤§é™åˆ¶: {len(actual)} > {max_length}")
                elif isinstance(actual, dict):
                    if len(actual) > max_length:
                        errors.append(f"è¾“å‡ºé¡¹æ•°è¶…è¿‡æœ€å¤§é™åˆ¶: {len(actual)} > {max_length}")

            if 'contains_keys' in expected and isinstance(actual, dict):
                # æ£€æŸ¥å­—å…¸æ˜¯å¦åŒ…å«æŒ‡å®šé”®
                missing_keys = set(expected['contains_keys']) - set(actual.keys())
                if missing_keys:
                    errors.append(f"è¾“å‡ºå­—å…¸ç¼ºå°‘é”®: {missing_keys}")

        except Exception as e:
            errors.append(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")

        return errors

    async def run_all_tests(self) -> List[TestResult]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        if not self.test_cases:
            print("[WARNING] æ²¡æœ‰æµ‹è¯•ç”¨ä¾‹å¯è¿è¡Œ")
            return []

        print(f"[INFO] å¼€å§‹è¿è¡Œ {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        self.test_results = []

        for i, test_case in enumerate(self.test_cases, 1):
            print(f"[INFO] è¿è¡Œæµ‹è¯• {i}/{len(self.test_cases)}: {test_case.name}")

            result = await self.run_single_test(test_case)
            self.test_results.append(result)

            # è¾“å‡ºæµ‹è¯•ç»“æœ
            status_symbol = {
                "passed": "âœ…",
                "failed": "âŒ",
                "timeout": "â°",
                "error": "ğŸ’¥"
            }.get(result.status, "â“")

            print(f"[{status_symbol}] {test_case.name}: {result.status} "
                  f"({result.duration:.2f}s)")

            if result.error_message:
                print(f"    é”™è¯¯: {result.error_message}")

            if result.assertion_errors:
                for error in result.assertion_errors:
                    print(f"    æ–­è¨€å¤±è´¥: {error}")

        return self.test_results

    def generate_test_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            return "æ²¡æœ‰æµ‹è¯•ç»“æœ"

        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.status == "passed")
        failed_tests = sum(1 for r in self.test_results if r.status == "failed")
        timeout_tests = sum(1 for r in self.test_results if r.status == "timeout")
        error_tests = sum(1 for r in self.test_results if r.status == "error")

        total_duration = sum(r.duration for r in self.test_results)
        avg_duration = total_duration / total_tests if total_tests > 0 else 0

        report = f"""
# LangGraphä»£ç†æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- **æ€»æµ‹è¯•æ•°**: {total_tests}
- **é€šè¿‡**: {passed_tests} ({passed_tests/total_tests*100:.1f}%)
- **å¤±è´¥**: {failed_tests} ({failed_tests/total_tests*100:.1f}%)
- **è¶…æ—¶**: {timeout_tests} ({timeout_tests/total_tests*100:.1f}%)
- **é”™è¯¯**: {error_tests} ({error_tests/total_tests*100:.1f}%)
- **æ€»è€—æ—¶**: {total_duration:.2f} ç§’
- **å¹³å‡è€—æ—¶**: {avg_duration:.2f} ç§’

## è¯¦ç»†ç»“æœ

### é€šè¿‡çš„æµ‹è¯•
{self._format_test_results([r for r in self.test_results if r.status == "passed"])}

### å¤±è´¥çš„æµ‹è¯•
{self._format_test_results([r for r in self.test_results if r.status == "failed"])}

### è¶…æ—¶çš„æµ‹è¯•
{self._format_test_results([r for r in self.test_results if r.status == "timeout"])}

### é”™è¯¯çš„æµ‹è¯•
{self._format_test_results([r for r in self.test_results if r.status == "error"])}

## æ€§èƒ½åˆ†æ
{self._analyze_performance()}

## å»ºè®®å’Œæ”¹è¿›
{self._generate_recommendations()}

---
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """

        return report.strip()

    def _format_test_results(self, results: List[TestResult]) -> str:
        """æ ¼å¼åŒ–æµ‹è¯•ç»“æœåˆ—è¡¨"""
        if not results:
            return "- æ— "

        formatted = []
        for result in results:
            line = f"- **{result.test_name}**: {result.duration:.2f}s"
            if result.error_message:
                line += f" - {result.error_message}"
            if result.assertion_errors:
                for error in result.assertion_errors[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªé”™è¯¯
                    line += f"\n  - {error}"
            formatted.append(line)

        return "\n".join(formatted)

    def _analyze_performance(self) -> str:
        """åˆ†ææµ‹è¯•æ€§èƒ½"""
        if not self.test_results:
            return "- æ²¡æœ‰æ€§èƒ½æ•°æ®"

        durations = [r.duration for r in self.test_results]
        avg_duration = sum(durations) / len(durations)
        max_duration = max(durations)
        min_duration = min(durations)

        slow_tests = [r for r in self.test_results if r.duration > avg_duration * 2]

        analysis = f"""
- **å¹³å‡æ‰§è¡Œæ—¶é—´**: {avg_duration:.2f} ç§’
- **æœ€å¿«æ‰§è¡Œæ—¶é—´**: {min_duration:.2f} ç§’
- **æœ€æ…¢æ‰§è¡Œæ—¶é—´**: {max_duration:.2f} ç§’
- **æ…¢é€Ÿæµ‹è¯•**: {len(slow_tests)} ä¸ª
        """

        if slow_tests:
            analysis += f"\n\n### æ…¢é€Ÿæµ‹è¯•\n"
            for test in sorted(slow_tests, key=lambda x: x.duration, reverse=True)[:5]:
                analysis += f"- {test.test_name}: {test.duration:.2f}s\n"

        return analysis.strip()

    def _generate_recommendations(self) -> str:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        failed_count = sum(1 for r in self.test_results if r.status == "failed")
        error_count = sum(1 for r in self.test_results if r.status == "error")
        timeout_count = sum(1 for r in self.test_results if r.status == "timeout")

        if failed_count > 0:
            recommendations.append(f"- æœ‰ {failed_count} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œæ£€æŸ¥é¢„æœŸè¾“å‡ºè®¾ç½®")

        if error_count > 0:
            recommendations.append(f"- æœ‰ {error_count} ä¸ªæµ‹è¯•å‡ºç°é”™è¯¯ï¼Œæ£€æŸ¥ä»£ç†å®ç°å’Œé”™è¯¯å¤„ç†")

        if timeout_count > 0:
            recommendations.append(f"- æœ‰ {timeout_count} ä¸ªæµ‹è¯•è¶…æ—¶ï¼Œè€ƒè™‘å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä¼˜åŒ–æ€§èƒ½")

        avg_duration = sum(r.duration for r in self.test_results) / len(self.test_results)
        if avg_duration > 5.0:
            recommendations.append("- å¹³å‡æµ‹è¯•æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘æ€§èƒ½ä¼˜åŒ–")

        if not recommendations:
            recommendations.append("- æ‰€æœ‰æµ‹è¯•è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒä»£ç è´¨é‡")

        return "\n".join(recommendations)

    def save_test_results(self, output_path: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        output_file = Path(output_path)

        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
        serializable_results = []
        for result in self.test_results:
            result_dict = asdict(result)
            if result.output:
                # ç®€åŒ–è¾“å‡ºä»¥ä¾¿JSONåºåˆ—åŒ–
                try:
                    result_dict['output'] = str(result.output)[:1000] + "..." if len(str(result.output)) > 1000 else str(result.output)
                except:
                    result_dict['output'] = "[æ— æ³•åºåˆ—åŒ–]"
            serializable_results.append(result_dict)

        test_data = {
            "test_summary": {
                "total_tests": len(self.test_results),
                "passed": sum(1 for r in self.test_results if r.status == "passed"),
                "failed": sum(1 for r in self.test_results if r.status == "failed"),
                "timeout": sum(1 for r in self.test_results if r.status == "timeout"),
                "error": sum(1 for r in self.test_results if r.status == "error"),
                "total_duration": sum(r.duration for r in self.test_results)
            },
            "test_results": serializable_results,
            "test_cases": [asdict(tc) for tc in self.test_cases]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, indent=2, ensure_ascii=False)

        print(f"[SUCCESS] æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def create_sample_test_cases(output_path: str):
    """åˆ›å»ºç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶"""
    sample_data = {
        "description": "LangGraphä»£ç†ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹",
        "version": "1.0",
        "test_cases": [
            {
                "name": "basic_conversation",
                "description": "åŸºç¡€å¯¹è¯æµ‹è¯•",
                "input_data": {
                    "messages": [{"role": "human", "content": "ä½ å¥½"}]
                },
                "expected_output": {
                    "contains": ["ä½ å¥½", "Hello"],
                    "type": "dict"
                },
                "timeout": 30.0,
                "category": "conversation"
            },
            {
                "name": "tool_usage_test",
                "description": "å·¥å…·ä½¿ç”¨æµ‹è¯•",
                "input_data": {
                    "messages": [{"role": "human", "content": "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ"}]
                },
                "expected_output": {
                    "contains": ["æ—¶é—´", "ç‚¹", "æ—¶"],
                    "min_length": 10
                },
                "timeout": 20.0,
                "category": "tools"
            },
            {
                "name": "calculation_test",
                "description": "è®¡ç®—åŠŸèƒ½æµ‹è¯•",
                "input_data": {
                    "messages": [{"role": "human", "content": "è®¡ç®— 123 + 456"}]
                },
                "expected_output": {
                    "contains": ["579"],
                    "type": "dict"
                },
                "timeout": 15.0,
                "category": "calculation"
            }
        ]
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False)

    print(f"[SUCCESS] ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹å·²åˆ›å»º: {output_path}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LangGraphä»£ç†æµ‹è¯•æ¡†æ¶")
    parser.add_argument("--graph", help="LangGraphå›¾æ¨¡å—è·¯å¾„")
    parser.add_argument("--test-cases", help="æµ‹è¯•ç”¨ä¾‹JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", default="./test_results.json",
                       help="æµ‹è¯•ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--report", action="store_true",
                       help="ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
    parser.add_argument("--create-sample", action="store_true",
                       help="åˆ›å»ºç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶")
    parser.add_argument("--sample-path", default="./sample_test_cases.json",
                       help="ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹è¾“å‡ºè·¯å¾„")

    args = parser.parse_args()

    print("=" * 60)
    print("LangGraphä»£ç†æµ‹è¯•æ¡†æ¶")
    print("=" * 60)

    if args.create_sample:
        create_sample_test_cases(args.sample_path)
        return

    if not args.graph:
        print("[ERROR] è¯·æŒ‡å®šå›¾æ¨¡å—è·¯å¾„ (--graph)")
        return

    try:
        # åŠ¨æ€å¯¼å…¥å›¾æ¨¡å—
        module_path = args.graph.replace('/', '.').replace('.py', '')
        graph_module = __import__(module_path, fromlist=['graph'])
        graph = getattr(graph_module, 'graph')

        print(f"[INFO] æˆåŠŸåŠ è½½å›¾æ¨¡å—: {module_path}")

    except Exception as e:
        print(f"[ERROR] åŠ è½½å›¾æ¨¡å—å¤±è´¥: {e}")
        return

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = AgentTester(graph)

    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    if args.test_cases:
        tester.add_test_cases_from_file(args.test_cases)
    else:
        # æ·»åŠ é»˜è®¤æµ‹è¯•ç”¨ä¾‹
        default_test_cases = [
            TestCase(
                name="default_test",
                description="é»˜è®¤æµ‹è¯•",
                input_data={"messages": [{"role": "human", "content": "æµ‹è¯•æ¶ˆæ¯"}]},
                timeout=30.0
            )
        ]
        for test_case in default_test_cases:
            tester.add_test_case(test_case)

    # è¿è¡Œæµ‹è¯•
    print("[INFO] å¼€å§‹è¿è¡Œæµ‹è¯•...")
    results = await tester.run_all_tests()

    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        print("\n" + "=" * 60)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        report = tester.generate_test_report()
        print(report)

    # ä¿å­˜ç»“æœ
    tester.save_test_results(args.output)

    # è¿”å›é€€å‡ºç 
    failed_count = sum(1 for r in results if r.status != "passed")
    if failed_count > 0:
        print(f"\n[WARNING] æœ‰ {failed_count} ä¸ªæµ‹è¯•æœªé€šè¿‡")
        sys.exit(1)
    else:
        print(f"\n[SUCCESS] æ‰€æœ‰ {len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
        sys.exit(0)


if __name__ == "__main__":
    asyncio.run(main())