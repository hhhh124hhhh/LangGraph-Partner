"""
AI Partner Chat æ™ºèƒ½ä½“
é›†æˆä¸ªæ€§åŒ–å¯¹è¯ã€å‘é‡åŒ–æ£€ç´¢å’Œè®°å¿†ç®¡ç†
"""

from typing import Dict, List, Optional, Any, TypedDict
from datetime import datetime
import json

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field

from utils.llm import CustomLLM
from utils.vector_store import VectorStore
from utils.persona_manager import PersonaManager
from utils.memory_manager import MemoryManager, ConversationTurn


class AgentState(BaseModel):
    """æ™ºèƒ½ä½“çŠ¶æ€å®šä¹‰"""
    user_message: str = Field(description="ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯")
    ai_response: str = Field(default="", description="AI ç”Ÿæˆçš„å›åº”")

    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    persona_context: str = Field(default="", description="ç”¨æˆ·å’ŒAIç”»åƒä¸Šä¸‹æ–‡")
    relevant_notes: List[Dict] = Field(default_factory=list, description="ç›¸å…³çš„å†å²ç¬”è®°")
    conversation_context: str = Field(default="", description="å¯¹è¯å†å²ä¸Šä¸‹æ–‡")

    # æ£€ç´¢å’Œæœç´¢ç»“æœ
    search_query: str = Field(default="", description="ç”¨äºæ£€ç´¢çš„æŸ¥è¯¢")
    retrieval_results: List[Dict] = Field(default_factory=list, description="æ£€ç´¢ç»“æœ")

    # å·¥å…·è°ƒç”¨ç›¸å…³
    tool_calls: List[Dict] = Field(default_factory=list, description="å·¥å…·è°ƒç”¨è®°å½•")
    tool_results: List[Dict] = Field(default_factory=list, description="å·¥å…·æ‰§è¡Œç»“æœ")

    # å…ƒæ•°æ®
    timestamp: datetime = Field(default_factory=datetime.now, description="æ—¶é—´æˆ³")
    session_id: str = Field(default="", description="ä¼šè¯ID")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…¶ä»–å…ƒæ•°æ®")


class AIPartnerAgent:
    """AI Partner æ™ºèƒ½ä½“"""

    def __init__(
        self,
        config_dir: str = "./config",
        vector_db_path: str = "./vector_db",
        memory_dir: str = "./memory"
    ):
        """
        åˆå§‹åŒ– AI Partner æ™ºèƒ½ä½“

        Args:
            config_dir: é…ç½®æ–‡ä»¶ç›®å½•
            vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            memory_dir: è®°å¿†å­˜å‚¨ç›®å½•
        """
        # åˆå§‹åŒ–ç»„ä»¶
        self.llm = CustomLLM()
        self.vector_store = VectorStore(vector_db_path)
        self.persona_manager = PersonaManager(config_dir)
        self.memory_manager = MemoryManager(memory_dir)

        # åˆ›å»ºæˆ–æ¢å¤ä¼šè¯
        if not self.memory_manager.current_session:
            self.memory_manager.create_session()
        
        # æ£€æŸ¥ç‚¹ä¿å­˜å™¨ï¼ˆç”¨äºçŠ¶æ€æŒä¹…åŒ–ï¼‰
        self.checkpointer = MemorySaver()

        # æ„å»ºçŠ¶æ€å›¾
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """æ„å»ºæ™ºèƒ½ä½“çŠ¶æ€å›¾"""

        # å®šä¹‰çŠ¶æ€å›¾
        workflow = StateGraph(AgentState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("load_context", self._load_context)
        workflow.add_node("analyze_query", self._analyze_query)
        workflow.add_node("search_notes", self._search_notes)
        workflow.add_node("call_tools", self._call_tools)
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("update_memory", self._update_memory)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("load_context")

        # æ·»åŠ è¾¹
        workflow.add_edge("load_context", "analyze_query")

        # æ¡ä»¶è¾¹ï¼šæ ¹æ®åˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥
        workflow.add_conditional_edges(
            "analyze_query",
            self._decide_next_step,
            {
                "search": "search_notes",
                "tools": "call_tools",
                "respond": "generate_response"
            }
        )

        workflow.add_edge("search_notes", "generate_response")
        workflow.add_edge("call_tools", "generate_response")
        workflow.add_edge("generate_response", "update_memory")
        workflow.add_edge("update_memory", END)

        return workflow.compile(checkpointer=self.checkpointer)

    async def _load_context(self, state: AgentState) -> AgentState:
        """åŠ è½½ç”»åƒå’Œå¯¹è¯ä¸Šä¸‹æ–‡"""
        print("ğŸ” åŠ è½½ç”»åƒå’Œä¸Šä¸‹æ–‡...")

        # åŠ è½½ç”»åƒä¸Šä¸‹æ–‡
        persona_context = self.persona_manager.get_persona_context()

        # åŠ è½½å¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = self.memory_manager.get_current_context(max_turns=3)

        # è·å–ä¼šè¯ä¿¡æ¯
        session_id = self.memory_manager.current_session.session_id

        state.persona_context = persona_context
        state.conversation_context = conversation_context
        state.session_id = session_id

        print(f"âœ… ä¸Šä¸‹æ–‡åŠ è½½å®Œæˆï¼Œä¼šè¯ID: {session_id}")
        return state

    async def _analyze_query(self, state: AgentState) -> AgentState:
        """åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­éœ€è¦æ‰§è¡Œçš„æ“ä½œ"""
        print(f"ğŸ§  åˆ†æç”¨æˆ·æŸ¥è¯¢: {state.user_message[:50]}...")

        user_message = state.user_message

        # åˆ¤æ–­æŸ¥è¯¢ç±»å‹
        query_lower = user_message.lower()

        # å·¥å…·è°ƒç”¨å…³é”®è¯
        tool_keywords = ["è®¡ç®—", "å¤©æ°”", "è®¡ç®—å™¨", "calculator", "weather"]
        # è®°å¿†æ£€ç´¢å…³é”®è¯
        memory_keywords = ["è®°å¾—", "ä¹‹å‰", "å†å²", "ç¬”è®°", "è®°å½•", "å›æƒ³èµ·"]

        needs_tools = any(keyword in query_lower for keyword in tool_keywords)
        needs_memory = any(keyword in query_lower for keyword in memory_keywords)

        # ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼ˆç”¨äºè¯­ä¹‰æ£€ç´¢ï¼‰
        search_query = user_message
        if len(user_message) > 100:
            # æˆªå–å…³é”®éƒ¨åˆ†
            search_query = user_message[:100] + "..."

        state.search_query = search_query

        # å†³å®šä¸‹ä¸€æ­¥æ“ä½œ
        if needs_tools:
            state.metadata["next_action"] = "tools"
            print("ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨éœ€æ±‚")
        elif needs_memory:
            state.metadata["next_action"] = "search"
            print("ğŸ“š æ£€æµ‹åˆ°è®°å¿†æ£€ç´¢éœ€æ±‚")
        else:
            state.metadata["next_action"] = "respond"
            print("ğŸ’¬ å‡†å¤‡ç”Ÿæˆå›åº”")

        return state

    def _decide_next_step(self, state: AgentState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥æ“ä½œ"""
        return state.metadata.get("next_action", "respond")

    async def _search_notes(self, state: AgentState) -> AgentState:
        """æœç´¢ç›¸å…³ç¬”è®°"""
        print(f"ğŸ” æœç´¢ç›¸å…³ç¬”è®°: {state.search_query[:50]}...")

        try:
            # æ‰§è¡Œè¯­ä¹‰æœç´¢
            search_results = self.vector_store.search(
                query=state.search_query,
                top_k=5,
                min_score=0.2
            )

            state.relevant_notes = search_results
            state.retrieval_results = search_results

            print(f"âœ… æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ç¬”è®°")

            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
            if search_results:
                notes_text = "ç›¸å…³ç¬”è®°å†…å®¹ï¼š\n"
                for i, note in enumerate(search_results):
                    notes_text += f"{i+1}. {note['content'][:200]}...\n"
                state.metadata["formatted_notes"] = notes_text

        except Exception as e:
            print(f"âŒ ç¬”è®°æœç´¢å¤±è´¥: {e}")
            state.relevant_notes = []

        return state

    async def _call_tools(self, state: AgentState) -> AgentState:
        """è°ƒç”¨å·¥å…·"""
        print("ğŸ”§ å‡†å¤‡è°ƒç”¨å·¥å…·...")

        # è¿™é‡Œå¯ä»¥é›†æˆç°æœ‰çš„å·¥å…·ç³»ç»Ÿ
        # æš‚æ—¶è¿”å›ç©ºç»“æœï¼Œåç»­å¯ä»¥æ‰©å±•
        state.tool_results = []
        state.metadata["tool_executed"] = False

        print("â„¹ï¸ å·¥å…·è°ƒç”¨åŠŸèƒ½å¾…å®ç°")
        return state

    async def _generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆä¸ªæ€§åŒ–å›åº”"""
        print("ğŸ’­ ç”ŸæˆAIå›åº”...")

        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []

        # ç”»åƒä¸Šä¸‹æ–‡
        if state.persona_context:
            context_parts.append(f"ç”»åƒä¿¡æ¯ï¼š\n{state.persona_context}")

        # å¯¹è¯ä¸Šä¸‹æ–‡
        if state.conversation_context:
            context_parts.append(f"æœ€è¿‘å¯¹è¯ï¼š\n{state.conversation_context}")

        # ç›¸å…³ç¬”è®°
        if state.relevant_notes:
            notes_summary = "ç›¸å…³å†å²ç¬”è®°ï¼š\n"
            for i, note in enumerate(state.relevant_notes[:3]):  # æœ€å¤šä½¿ç”¨3æ¡ç¬”è®°
                notes_summary += f"{i+1}. {note['content'][:150]}...\n"
            context_parts.append(notes_summary)

        # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡
        full_context = "\n\n".join(context_parts) if context_parts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"

        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå¼€å‘ä¼™ä¼´ï¼Œæ“…é•¿LangGraphæ¡†æ¶å’Œæ™ºèƒ½ä½“å¼€å‘ã€‚

{full_context}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œæä¾›ä¸ªæ€§åŒ–ã€ä¸“ä¸šçš„å›åº”ã€‚æ³¨æ„ï¼š
1. ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­è°ƒ
2. è‡ªç„¶åœ°å¼•ç”¨ç›¸å…³çš„å†å²ç»éªŒå’Œç¬”è®°
3. æä¾›å®ç”¨çš„å»ºè®®å’Œä»£ç ç¤ºä¾‹
4. ä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥
5. å¦‚æœç”¨æˆ·è¯¢é—®æŠ€æœ¯é—®é¢˜ï¼Œç»™å‡ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆ

ç”¨æˆ·æ¶ˆæ¯ï¼š{state.user_message}"""

        try:
            # ç”Ÿæˆå›åº”
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=state.user_message)
            ]

            response = self.llm.invoke(messages)
            
            # ç¡®ä¿æ­£ç¡®è·å–content
            if isinstance(response, dict) and "content" in response:
                ai_response = response["content"]
            elif hasattr(response, 'content'):
                ai_response = response.content
            else:
                # å…¼å®¹å…¶ä»–å¯èƒ½çš„è¿”å›æ ¼å¼
                ai_response = str(response)

            state.ai_response = ai_response
            print(f"âœ… å›åº”ç”Ÿæˆå®Œæˆ: {ai_response[:100]}...")

        except Exception as e:
            print(f"âŒ å›åº”ç”Ÿæˆå¤±è´¥: {e}")
            state.ai_response = "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›åº”æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·ç¨åå†è¯•ã€‚"

        return state

    async def _update_memory(self, state: AgentState) -> AgentState:
        """æ›´æ–°è®°å¿†ç³»ç»Ÿ"""
        print("ğŸ’¾ æ›´æ–°è®°å¿†ç³»ç»Ÿ...")

        try:
            # æ·»åŠ å¯¹è¯è½®æ¬¡åˆ°è®°å¿†
            context_used = []

            # è®°å½•ä½¿ç”¨çš„ä¸Šä¸‹æ–‡
            if state.relevant_notes:
                context_used.extend([f"ç¬”è®°: {note['content'][:50]}..." for note in state.relevant_notes])

            if state.persona_context:
                context_used.append("ç”¨æˆ·ç”»åƒä¿¡æ¯")

            success = self.memory_manager.add_conversation_turn(
                user_message=state.user_message,
                ai_response=state.ai_response,
                context_used=context_used,
                tools_called=state.tool_calls,
                search_query=state.search_query,
                retrieval_count=len(state.relevant_notes)
            )

            if success:
                print("âœ… è®°å¿†æ›´æ–°æˆåŠŸ")
            else:
                print("âŒ è®°å¿†æ›´æ–°å¤±è´¥")

        except Exception as e:
            print(f"âŒ è®°å¿†æ›´æ–°å¼‚å¸¸: {e}")

        return state

    async def chat(self, user_message: str) -> str:
        """
        è¿›è¡Œå¯¹è¯

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯

        Returns:
            AI å›åº”
        """
        print(f"\nğŸ¯ ç”¨æˆ·æ¶ˆæ¯: {user_message}")
        print("-" * 50)

        try:
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = AgentState(
                user_message=user_message,
                timestamp=datetime.now()
            )

            # æ‰§è¡ŒçŠ¶æ€å›¾
            config = {"configurable": {"thread_id": self.memory_manager.current_session.session_id}}
            result = await self.graph.ainvoke(initial_state, config=config)

            # å¤„ç†ä¸åŒç±»å‹çš„è¿”å›å€¼
            if isinstance(result, dict):
                return result.get("ai_response", "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å›åº”ã€‚")
            elif hasattr(result, "ai_response"):
                return result.ai_response
            else:
                return "æŠ±æ­‰ï¼Œå›åº”æ ¼å¼ä¸æ­£ç¡®ã€‚"

        except Exception as e:
            error_msg = f"å¯¹è¯å¤„ç†å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°äº†é”™è¯¯ï¼š{error_msg}"

    def get_session_info(self) -> Dict:
        """è·å–å½“å‰ä¼šè¯ä¿¡æ¯"""
        return {
            "session_id": self.memory_manager.current_session.session_id,
            "vector_store_stats": self.vector_store.get_stats(),
            "memory_stats": self.memory_manager.get_memory_stats(),
            "persona_validation": self.persona_manager.validate_persona_files()
        }

    async def close(self):
        """å…³é—­æ™ºèƒ½ä½“ï¼Œæ¸…ç†èµ„æº"""
        print("ğŸ”š å…³é—­AI Partneræ™ºèƒ½ä½“...")

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        self.memory_manager._save_memory()

        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")


# ä¾¿æ·å‡½æ•°
async def create_partner_agent(
    config_dir: str = "./config",
    vector_db_path: str = "./vector_db",
    memory_dir: str = "./memory"
) -> AIPartnerAgent:
    """
    åˆ›å»ºAI Partneræ™ºèƒ½ä½“å®ä¾‹

    Args:
        config_dir: é…ç½®ç›®å½•
        vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
        memory_dir: è®°å¿†ç›®å½•

    Returns:
        æ™ºèƒ½ä½“å®ä¾‹
    """
    return AIPartnerAgent(config_dir, vector_db_path, memory_dir)


if __name__ == "__main__":
    import asyncio

    async def demo():
        agent = await create_partner_agent()

        # ç¤ºä¾‹å¯¹è¯
        test_messages = [
            "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹LangGraphçš„åŸºæœ¬ç”¨æ³•",
            "è®°å¾—æˆ‘ä¹‹å‰æåˆ°çš„Cozeé¡¹ç›®å—ï¼Ÿæˆ‘æƒ³ç”¨LangGraphé‡æ–°å®ç°",
            "ä½ èƒ½å¸®æˆ‘è®¾è®¡ä¸€ä¸ªæ™ºèƒ½ä½“çš„æ¶æ„å—ï¼Ÿ"
        ]

        for msg in test_messages:
            response = await agent.chat(msg)
            print(f"\nAIå›åº”: {response}")
            print("\n" + "="*80 + "\n")

        await agent.close()

    asyncio.run(demo())