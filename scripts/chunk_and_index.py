"""
æ™ºèƒ½ç¬”è®°åˆ†å—å’Œç´¢å¼•è„šæœ¬
åŸºäº AI Agent æ–¹æ³•ï¼ŒåŠ¨æ€åˆ†æç¬”è®°æ ¼å¼å¹¶ç”Ÿæˆæœ€ä¼˜åˆ†å—ç­–ç•¥
"""

import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import re

# æ·»åŠ  AI partner chat è„šæœ¬è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / ".claude" / "ai-partner-chat" / "scripts"))

from chunk_schema import Chunk, validate_chunk
from vector_indexer import VectorIndexer


class IntelligentNoteChunker:
    """æ™ºèƒ½ç¬”è®°åˆ†å—å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–åˆ†å—å™¨"""
        self.chunk_strategies = {
            'date_entry': self._chunk_date_entry,
            'section_based': self._chunk_section_based,
            'paragraph_based': self._chunk_paragraph_based,
            'mixed_content': self._chunk_mixed_content
        }

    def analyze_note_format(self, content: str) -> str:
        """
        åˆ†æç¬”è®°æ ¼å¼ï¼Œç¡®å®šæœ€é€‚åˆçš„åˆ†å—ç­–ç•¥

        Args:
            content: ç¬”è®°å†…å®¹

        Returns:
            åˆ†å—ç­–ç•¥ç±»å‹
        """
        lines = content.split('\n')

        # æ£€æµ‹æ—¥æœŸæ¡ç›®æ ¼å¼ï¼ˆå¦‚ï¼š2024-01-01, ## 2024å¹´1æœˆ1æ—¥ï¼‰
        date_pattern = re.compile(r'^\d{4}-\d{2}-\d{2}|^\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|^##\s*\d{4}')
        date_lines = sum(1 for line in lines if date_pattern.match(line.strip()))

        if date_lines > 1:
            return 'date_entry'

        # æ£€æµ‹ç« èŠ‚æ ‡é¢˜æ ¼å¼ï¼ˆå¦‚ï¼š#, ##, ###ï¼‰
        header_pattern = re.compile(r'^#+\s+')
        header_lines = sum(1 for line in lines if header_pattern.match(line.strip()))

        # æ£€æµ‹åˆ—è¡¨é¡¹æ ¼å¼
        list_pattern = re.compile(r'^\s*[-*+]\s+|^\s*\d+\.\s+')
        list_lines = sum(1 for line in lines if list_pattern.match(line.strip()))

        if header_lines > 2:
            return 'section_based'
        elif list_lines > len(lines) * 0.3:
            return 'mixed_content'
        else:
            return 'paragraph_based'

    def _chunk_date_entry(self, content: str, filepath: str) -> List[Chunk]:
        """æŒ‰æ—¥æœŸæ¡ç›®åˆ†å—"""
        chunks = []
        lines = content.split('\n')
        current_chunk_lines = []
        chunk_id = 0

        # æ—¥æœŸæ¨¡å¼
        date_pattern = re.compile(r'^(\d{4}-\d{2}-\d{2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|##\s*\d{4})')

        for line in lines:
            # æ£€æµ‹æ–°çš„æ—¥æœŸæ¡ç›®
            if date_pattern.match(line.strip()) and current_chunk_lines:
                # ä¿å­˜å½“å‰å—
                chunk_content = '\n'.join(current_chunk_lines).strip()
                if chunk_content:
                    chunks.append(self._create_chunk(
                        chunk_content, filepath, chunk_id, 'date_entry'
                    ))
                    chunk_id += 1
                current_chunk_lines = [line]
            else:
                current_chunk_lines.append(line)

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk_lines:
            chunk_content = '\n'.join(current_chunk_lines).strip()
            if chunk_content:
                chunks.append(self._create_chunk(
                    chunk_content, filepath, chunk_id, 'date_entry'
                ))

        return chunks

    def _chunk_section_based(self, content: str, filepath: str) -> List[Chunk]:
        """åŸºäºç« èŠ‚æ ‡é¢˜åˆ†å—"""
        chunks = []
        lines = content.split('\n')
        current_chunk_lines = []
        chunk_id = 0
        current_title = ""

        # ç« èŠ‚æ ‡é¢˜æ¨¡å¼
        header_pattern = re.compile(r'^(#+)\s+(.+)')

        for line in lines:
            header_match = header_pattern.match(line.strip())

            if header_match and current_chunk_lines:
                # ä¿å­˜å½“å‰ç« èŠ‚
                chunk_content = '\n'.join(current_chunk_lines).strip()
                if chunk_content:
                    chunks.append(self._create_chunk(
                        chunk_content, filepath, chunk_id, 'section',
                        title=current_title
                    ))
                    chunk_id += 1

                current_chunk_lines = [line]
                current_title = header_match.group(2).strip()
            else:
                current_chunk_lines.append(line)

        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_chunk_lines:
            chunk_content = '\n'.join(current_chunk_lines).strip()
            if chunk_content:
                chunks.append(self._create_chunk(
                    chunk_content, filepath, chunk_id, 'section',
                    title=current_title
                ))

        return chunks

    def _chunk_paragraph_based(self, content: str, filepath: str) -> List[Chunk]:
        """åŸºäºæ®µè½åˆ†å—"""
        chunks = []
        paragraphs = content.split('\n\n')
        chunk_id = 0

        current_chunk_paragraphs = []
        current_length = 0

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # å¦‚æœå½“å‰å—å¤ªé•¿ï¼Œåˆ›å»ºæ–°å—
            if current_length + len(paragraph) > 800 and current_chunk_paragraphs:
                chunk_content = '\n\n'.join(current_chunk_paragraphs)
                chunks.append(self._create_chunk(
                    chunk_content, filepath, chunk_id, 'paragraph'
                ))
                chunk_id += 1

                current_chunk_paragraphs = [paragraph]
                current_length = len(paragraph)
            else:
                current_chunk_paragraphs.append(paragraph)
                current_length += len(paragraph)

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk_paragraphs:
            chunk_content = '\n\n'.join(current_chunk_paragraphs)
            chunks.append(self._create_chunk(
                chunk_content, filepath, chunk_id, 'paragraph'
            ))

        return chunks

    def _chunk_mixed_content(self, content: str, filepath: str) -> List[Chunk]:
        """æ··åˆå†…å®¹åˆ†å—ï¼ˆå¤„ç†åŒ…å«åˆ—è¡¨ã€æ ‡é¢˜ç­‰çš„å¤æ‚å†…å®¹ï¼‰"""
        chunks = []
        lines = content.split('\n')
        current_chunk_lines = []
        chunk_id = 0
        current_title = ""

        # æ ‡é¢˜å’Œåˆ—è¡¨æ¨¡å¼
        header_pattern = re.compile(r'^(#+)\s+(.+)')
        list_pattern = re.compile(r'^\s*[-*+]\s+|^\s*\d+\.\s+')

        for i, line in enumerate(lines):
            line = line.strip()

            # æ£€æµ‹æ ‡é¢˜
            header_match = header_pattern.match(line)
            if header_match:
                # å¦‚æœæœ‰å†…å®¹ accumulatedï¼Œå…ˆä¿å­˜
                if current_chunk_lines:
                    chunk_content = '\n'.join(current_chunk_lines).strip()
                    if chunk_content:
                        chunks.append(self._create_chunk(
                            chunk_content, filepath, chunk_id, 'mixed_content',
                            title=current_title
                        ))
                        chunk_id += 1

                current_chunk_lines = [line]
                current_title = header_match.group(2).strip()
            # ç©ºè¡Œä¸”å½“å‰å—æœ‰å†…å®¹æ—¶ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªæ–°éƒ¨åˆ†çš„å¼€å§‹
            elif not line and current_chunk_lines and len(current_chunk_lines) > 3:
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯æ ‡é¢˜æˆ–åˆ—è¡¨
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if (header_pattern.match(next_line) or
                        list_pattern.match(next_line)):
                        # ä¿å­˜å½“å‰å—
                        chunk_content = '\n'.join(current_chunk_lines).strip()
                        if chunk_content:
                            chunks.append(self._create_chunk(
                                chunk_content, filepath, chunk_id, 'mixed_content',
                                title=current_title
                            ))
                            chunk_id += 1
                        current_chunk_lines = []
                        current_title = ""
                        continue

            # æ£€æŸ¥å½“å‰å—æ˜¯å¦å¤ªé•¿
            if len('\n'.join(current_chunk_lines + [line])) > 1000:
                chunk_content = '\n'.join(current_chunk_lines).strip()
                if chunk_content:
                    chunks.append(self._create_chunk(
                        chunk_content, filepath, chunk_id, 'mixed_content',
                        title=current_title
                    ))
                    chunk_id += 1
                current_chunk_lines = [line] if line else []
            else:
                current_chunk_lines.append(line)

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk_lines:
            chunk_content = '\n'.join(current_chunk_lines).strip()
            if chunk_content:
                chunks.append(self._create_chunk(
                    chunk_content, filepath, chunk_id, 'mixed_content',
                    title=current_title
                ))

        return chunks

    def _create_chunk(
        self,
        content: str,
        filepath: str,
        chunk_id: int,
        chunk_type: str,
        title: str = None,
        date: str = None
    ) -> Chunk:
        """åˆ›å»ºæ ‡å‡†æ ¼å¼çš„æ–‡æ¡£å—"""
        # æå–æ—¥æœŸä¿¡æ¯
        if not date:
            date_match = re.search(r'(\d{4}-\d{2}-\d{2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', content)
            date = date_match.group(1) if date_match else None

        metadata = {
            'filename': Path(filepath).name,
            'filepath': filepath,
            'chunk_id': chunk_id,
            'chunk_type': chunk_type,
            'title': title,
            'date': date
        }

        chunk = {
            'content': content,
            'metadata': metadata
        }

        # éªŒè¯å—æ ¼å¼
        if not validate_chunk(chunk):
            print(f"è­¦å‘Šï¼šç”Ÿæˆçš„å—æ ¼å¼ä¸æ­£ç¡®: {metadata}")

        return chunk

    def chunk_note_file(self, filepath: str) -> List[Chunk]:
        """
        åˆ†æå¹¶åˆ†å—æŒ‡å®šçš„ç¬”è®°æ–‡ä»¶

        Args:
            filepath: ç¬”è®°æ–‡ä»¶è·¯å¾„

        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            if not content.strip():
                print(f"è­¦å‘Šï¼šæ–‡ä»¶ {filepath} ä¸ºç©º")
                return []

            print(f"ğŸ“– åˆ†ææ–‡ä»¶: {filepath}")

            # åˆ†ææ ¼å¼å¹¶é€‰æ‹©åˆ†å—ç­–ç•¥
            strategy = self.analyze_note_format(content)
            print(f"ğŸ“‹ é€‰æ‹©åˆ†å—ç­–ç•¥: {strategy}")

            # æ‰§è¡Œåˆ†å—
            chunker_func = self.chunk_strategies.get(strategy, self._chunk_paragraph_based)
            chunks = chunker_func(content, filepath)

            print(f"âœ… ç”Ÿæˆäº† {len(chunks)} ä¸ªå—")
            return chunks

        except Exception as e:
            print(f"âŒ åˆ†å—æ–‡ä»¶ {filepath} å¤±è´¥: {e}")
            return []


def main():
    """ä¸»å‡½æ•°ï¼šåˆ†å—å¹¶ç´¢å¼•æ‰€æœ‰ç¬”è®°"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½ç¬”è®°åˆ†å—å’Œç´¢å¼•...")

    # åˆå§‹åŒ–
    chunker = IntelligentNoteChunker()
    indexer = VectorIndexer(db_path="./vector_db")
    indexer.initialize_db()

    # æŸ¥æ‰¾æ‰€æœ‰ç¬”è®°æ–‡ä»¶
    notes_dir = Path("./notes")
    if not notes_dir.exists():
        print("âŒ ç¬”è®°ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º ./notes ç›®å½•å¹¶æ·»åŠ  markdown æ–‡ä»¶")
        return

    # è·å–æ‰€æœ‰ markdown æ–‡ä»¶
    note_files = list(notes_dir.glob("**/*.md"))
    if not note_files:
        print("âŒ æœªæ‰¾åˆ° markdown æ–‡ä»¶ï¼Œè¯·åœ¨ ./notes ç›®å½•ä¸­æ·»åŠ  .md æ–‡ä»¶")
        return

    print(f"ğŸ“ æ‰¾åˆ° {len(note_files)} ä¸ªç¬”è®°æ–‡ä»¶")

    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    all_chunks = []
    for note_file in note_files:
        chunks = chunker.chunk_note_file(str(note_file))
        all_chunks.extend(chunks)

    if not all_chunks:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„å—")
        return

    print(f"ğŸ“¦ æ€»å…±ç”Ÿæˆäº† {len(all_chunks)} ä¸ªå—")

    # ç´¢å¼•å—
    print("ğŸ” å¼€å§‹å‘é‡ç´¢å¼•...")
    try:
        indexer.index_chunks(all_chunks)
        print("âœ… ç´¢å¼•å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
        return

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    try:
        stats = indexer.get_stats()
        print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"   æ€»å—æ•°: {stats['total_chunks']}")
        print(f"   é›†åˆå: {stats['collection_name']}")
        print(f"   æ•°æ®åº“è·¯å¾„: {stats['db_path']}")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯: {e}")

    print("\nğŸ‰ ç¬”è®°åˆ†å—å’Œç´¢å¼•å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥ä½¿ç”¨ AI Partner Chat è¿›è¡Œä¸ªæ€§åŒ–å¯¹è¯äº†ã€‚")


if __name__ == "__main__":
    main()